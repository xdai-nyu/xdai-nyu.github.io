# KnowCheck
**KnowCheck: An AI-Powered Knowledge Mastery Tool for Adult Learners**

---

## Problem Statement
Many adult learners studying AI concepts rely on scattered resources (online courses, podcasts, articles, and workplace training). While they may feel confident, research shows that self-assessment often fails to match actual ability—especially when it comes to applying or transferring knowledge.  

Most current assessments focus on memorization, not on mastery. Learners need a reliable way to measure true understanding and practical transfer of knowledge, not just recall.

---

## Target Learning Audience
KnowCheck is designed for young and mid-career adult learners, typically between **18–45 years old**, who are adapting to the rapid changes brought by AI.  

This group includes professionals and lifelong learners who are often self-studying AI topics while balancing work, study, and personal responsibilities.  

Unlike traditional students, they rarely have access to structured programs or consistent mentorship. They self-study topics such as **LLM principles**, **prompt engineering**, and **applied AI use in business settings**, pulling resources from fragmented sources like online courses, podcasts, and in-house trainings.  

Their learning is often fragmented and pragmatic—focused on “making it work” rather than deeply verifying understanding.  

**KnowCheck** provides a clear, context-independent way to check mastery and build confidence in applying knowledge in real-world scenarios.

---

## Identified Learning Need
For adults trying to learn AI on their own, it can be surprisingly difficult to know whether they’ve really mastered the material.  

Research shows that learners often **misjudge their own abilities**; in particular, low performers tend to overestimate their competence—a pattern described by the **Dunning–Kruger effect** (Kruger & Dunning, 1999).  

This issue is amplified in **fragmented, informal learning environments**, where adults rely on scattered resources such as online tutorials, podcasts, or in-house training.  

In these settings, learners may feel confident, but their performance often falls short when tested in real-world scenarios (Mahmood, 2016).  

Most current assessments are limited to **recall**, while professional contexts demand **application** and **transfer**. Without clear benchmarks or feedback, learners often mistake **recognition for mastery**.  

An **objective tool** with **targeted feedback** can help adults better calibrate their self-assessment and guide future learning (Lu et al., 2021).

---

## Rationale for AI Assistance
Traditional knowledge assessments, such as standardized tests or course-embedded exercises, often focus on evaluating a learner's ability to *recall* and *restate* information.  

However, for the adult self-learners we are targeting, the real challenge lies in their ability to *apply* knowledge in typical scenarios and, more importantly, *transfer* it to atypical, complex work situations.  

This higher-order competence is difficult to measure effectively with static, predefined question banks, which is precisely where **AI** can play a pivotal role.

---

### Personalization and Adaptive Feedback
AI/intelligent tutoring systems can generate scaffolded tasks and provide adaptive feedback based on the learner's proficiency and goals, with long-term evidence supporting their learning gains (vanLehn, 2011).  

In our design, this adaptability is specifically embodied by AI's capability to dynamically generate high-fidelity work scenarios to accurately assess a learner's true ability to *apply* and *transfer* knowledge.  

The feedback provided is not merely a judgment of correctness but also acts like a mentor, offering “why” explanations that pinpoint potential risks or logical fallacies in the user's proposed solutions.  

This constitutes the core of **explainable diagnostic feedback**.

---

### Usability of LLMs in Education
A recent survey by Wang et al. (2024) summarizes the applications of **LLMs in education**, including conceptual understanding detection, personalized guidance, and automated assessment, offering a direct reference for our design.  

AI can not only be used to evaluate performance but also to explain a user's cognitive blind spots and recommend the next steps in their learning path accordingly.  

This approach focuses on helping learners **calibrate their self-perception of competence**.  

By leveraging AI to create a highly realistic, interactive, and feedback-rich assessment environment, our tool aims to facilitate **cognitive self-calibration** for adult learners.  

This effectively helps them recognize their own capability boundaries and supports them in making the developmental leap from “knowing” to “doing.”

---

## Approach and Deliverables

We have chosen AI/LLM as the initial subject matter for our tool, focusing on the following assessment dimensions:

- **Restate:** The ability to explain a concept and its boundaries in one's own words.  
- **Apply:** The ability to use a concept correctly in typical scenarios.  
- **Transfer:** The ability to propose viable strategies in atypical or constrained scenarios.  
- **Evaluation:** The system will output a multi-layered score (covering recall, application, and transfer), a calibration map, an evidence package, and a supplementary learning path.  
- **Scope:** This tool does not provide courses or question banks; it is positioned as an independent mastery assessment layer to be used after learning has taken place.

---

## Team Contribution
- **Chenyu** drafted the initial version.  
- **Nanxi** revised the draft and incorporated changes.  
- **Sixin** finalized the writing and editing.


---

## References
- Lu, F.-I., Takahashi, S. G., & Kerr, C. (2021). *Myth or reality: Self-assessment is central to effective curriculum in graduate medical education.* **Academic Pathology, 8, 23742895211013528.**  
- Mahmood, K. (2016). *Do people overestimate their information literacy skills? A systematic review of empirical evidence on the Dunning–Kruger effect.* **Communications in Information Literacy, 10(2), 198–213.**  
- Wang, S., Xu, T., Li, H., Zhang, C., Liang, J., Tang, J., Yu, P. S., & Wen, Q. (2024). *Large Language Models for Education: A Survey and Outlook.* **arXiv:2403.18105.**  
- Kruger, J., & Dunning, D. (1999). *Unskilled and unaware of it: How difficulties in recognizing one’s own incompetence lead to inflated self-assessments.* **Journal of Personality and Social Psychology, 77(6), 1121–1134.**  
- vanLehn, K. (2011). *The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems.* **Educational Psychologist, 46(4), 197–221.**
